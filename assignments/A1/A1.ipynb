{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEd-mtF0PTrv"
      },
      "source": [
        "# Assignment 1\n",
        "You should submit the **UniversityNumber.ipynb** file and your final prediction file **UniversityNumber.test.out** to Moodle. Make sure your code does not use your local files and that the results are reproducible. Before submitting, please **run your notebook and keep all running logs** so that we can check."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8c9YBduQCI4"
      },
      "source": [
        "## 1 $n$-gram Language Model\n",
        "**Q1**: Expand the above definition of $ p(\\vec{w})$ using naive estimates of the parameters, such as $  p(w_4 \\mid w_2, w_3) \\stackrel{\\tiny{\\mbox{def}}}{=}  \\frac{C(w_2~w_3~w_4)}{C(w_2~w_3)} $ where \\( C(w_2 w_3 w_4) \\) denotes the count of times the trigram $ w_2 w_3 w_4 $ was observed in a training corpus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMQ_Z1g8QZef"
      },
      "source": [
        "**Write your answer:**\n",
        "\n",
        "\\# todo\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmgExbf1QtCH"
      },
      "source": [
        "**Q2**: One could also define a kind of reversed trigram language model $p_{reversed}$ that instead assumed the words were generated in reverse order (from right to left):\n",
        "\\begin{align} p_{reversed}(\\vec{w}) \\stackrel{\\tiny{\\mbox{def}}}{=}&p(w_n) \\cdot p(w_{n-1} \\mid w_n) \\cdot p(w_{n-2} \\mid w_{n-1} w_n) \\cdot p(w_{n-3} \\mid w_{n-2} w_{n-1}) \\\\ &\\cdots p(w_2 \\mid w_3 w_4) \\cdot p(w_1 \\mid w_2 w_3) \\end{align}\n",
        "By manipulating the notation, show that the two models are identical, i.e., $ p(\\vec{w}) = p_{reversed}(\\vec{w}) $ for any $ \\vec{w} $ provided that both models use MLE parameters estimated from the same training data (see Q1 above)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qm1ZGFIaRPCP"
      },
      "source": [
        "**Write your answer:**\n",
        "\n",
        "\\# todo\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQEc5kz4RniG"
      },
      "source": [
        "## 2 $N$-gram Language Model Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3kSwtN79jWgp"
      },
      "outputs": [],
      "source": [
        "!wget -O train.txt https://raw.githubusercontent.com/qtli/COMP7607-Fall2023/master/assignments/A1/data/lm/train.txt\n",
        "!wget -O dev.txt https://raw.githubusercontent.com/qtli/COMP7607-Fall2023/master/assignments/A1/data/lm/dev.txt\n",
        "!wget -O test.txt https://raw.githubusercontent.com/qtli/COMP7607-Fall2023/master/assignments/A1/data/lm/test.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9HCVQwqkTc_"
      },
      "source": [
        "### 2.1 Building vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KhFKCzwkaTn"
      },
      "source": [
        "**Code**\n",
        "\n",
        "\\# todo\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import re\n",
        "import numpy as np\n",
        "from functools import reduce\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = 'train'\n",
        "\n",
        "tokenized_data = []\n",
        "tokenized_data_2dim = []\n",
        "with open(f'data/lm/{dataset}.txt', encoding='utf-8') as fd:\n",
        "    lines = fd.readlines()\n",
        "    for line in lines:\n",
        "        words = ['<s>'] + re.split(r'\\s+', line.strip()) + ['</s>']\n",
        "        tokenized_data.extend(words)\n",
        "        tokenized_data_2dim.append(words)\n",
        "        \n",
        "# Step 1: Count token frequencies\n",
        "token_counts = Counter(tokenized_data)\n",
        "\n",
        "# Step 2: Replace infrequent tokens with \"<UNK>\"\n",
        "min_token_freq = 3\n",
        "tokenized_data_with_unk = [token if token_counts[token] >= min_token_freq else '<UNK>' for token in tokenized_data]\n",
        "tokenized_data_with_unk_2dim = [[token if token_counts[token] >= min_token_freq else '<UNK>' for token in line] for line in tokenized_data_2dim]\n",
        "\n",
        "# Step 3: Add start-of-sentence and end-of-sentence tokens\n",
        "vocabulary = list(set(tokenized_data_with_unk))\n",
        "vocabulary_size = len(vocabulary)\n",
        "\n",
        "print(\"Vocabulary size:\", vocabulary_size)\n",
        "# print('<UNK>' in vocabulary)\n",
        "# print(len(token_counts))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # for bigram\n",
        "# from collections import defaultdict\n",
        "# biCnt = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "# # To calculate the occurrence count of wi-1 wi\n",
        "# with open(f'data/lm/{dataset}.txt', encoding='utf-8') as fd:\n",
        "#     lines = fd.readlines()\n",
        "#     for line in lines:\n",
        "#         words = re.split(r'\\s+', line.strip())\n",
        "#         for i in range(1, len(words)):\n",
        "#             if words[i - 1] in unk:\n",
        "#                 words[i - 1] = '<UNK>'\n",
        "#             if words[i] in unk:\n",
        "#                 words[i] = '<UNK>'\n",
        "#             if words[i] == '':\n",
        "#                 print(words)\n",
        "#             biCnt[words[i - 1]][words[i]] += 1\n",
        "#         biCnt['<s>'][words[0]] += 1\n",
        "#         biCnt[words[-1]]['</s>'] += 1\n",
        "        \n",
        "# print(len(biCnt))\n",
        "# # assert len(biCnt) == len(uniCnt) - 1\n",
        "# # assert sum(biCnt['<s>'].values()) == uniCnt['<s>']\n",
        "# print(biCnt['<s>'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLQNsUA5kfZe"
      },
      "source": [
        "**Discussion**\n",
        "\n",
        "unigram:\n",
        "\n",
        "the number of parameters: Vocabulary size 19349\n",
        "\n",
        "bigram:\n",
        "\n",
        "the number of parameters: Vocabulary size^2 19349**2\n",
        "\n",
        "trigram:\n",
        "\n",
        "the number of parameters: Vocabulary size^3 19349**3\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJzDNMVikkeX"
      },
      "source": [
        "### 2.2 $N$-gram Language Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xxkcs2HykuR2"
      },
      "source": [
        "**Code**\n",
        "\n",
        "\\# todo\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "unigram\n",
        "\"\"\"\n",
        "unigram_counts = Counter(tokenized_data_with_unk)\n",
        "\n",
        "def calculate_unigram_prob(word):\n",
        "    return unigram_counts[word] / len(tokenized_data_with_unk)\n",
        "\n",
        "# Step 4: Calculate perplexity\n",
        "def calculate_perplexity(data):\n",
        "    num_words = len(data)\n",
        "    log_prob_sum = 0\n",
        "\n",
        "    for i in range(num_words):\n",
        "        word = data[i]\n",
        "        prob = calculate_unigram_prob(word)\n",
        "        log_prob_sum += np.log(prob)\n",
        "\n",
        "    perplexity = np.exp(log_prob_sum) ** (-1/num_words)\n",
        "    return perplexity\n",
        "\n",
        "# Calculate perplexity for each sentence\n",
        "sentence_perplexities = []\n",
        "for i, sentence in enumerate(tokenized_data_with_unk_2dim):\n",
        "    perplexity = calculate_perplexity(sentence)\n",
        "    sentence_perplexities.append(perplexity)\n",
        "\n",
        "\n",
        "print(\"Perplexity for each sentence:\")\n",
        "for i, perplexity in enumerate(sentence_perplexities):\n",
        "    print(f\"Sentence {i+1}: {perplexity}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # unigram\n",
        "# class Unigram():\n",
        "#     def train(self, data):\n",
        "#         pass\n",
        "\n",
        "# # calc prob for unigram\n",
        "# prob_d = {}\n",
        "# for word, cnt in voc.items():\n",
        "#     prob_d[word] = cnt / word_cnt\n",
        "# print(prob_d['<s>'])\n",
        "# print(prob_d['unk'])\n",
        "\n",
        "# # prob for all\n",
        "# # perp = 1\n",
        "# # for prob in prob_d.values():\n",
        "# #     perp *= prob ** (-1/len(prob_d))\n",
        "\n",
        "# # Calculate the perp of each sentence and then calculate the mean\n",
        "# perps = []\n",
        "# with open('data/lm/dev.txt', encoding='utf-8') as fd:\n",
        "#     lines = fd.readlines()\n",
        "#     for line in lines:\n",
        "#         words = line.split(' ')\n",
        "#         perp = 1\n",
        "#         for word in words:\n",
        "#             if word in prob_d:\n",
        "#                 perp *= prob_d[word] ** (-1/len(words))\n",
        "#             else:\n",
        "#                 perp *= prob_d['unk'] ** (-1/len(words))\n",
        "#         # perp = perp ** (-1/len(words))\n",
        "#         perps.append(perp)\n",
        "#         # print(perp)\n",
        "# minPerp = sum(perps) / len(perps)\n",
        "# minPerp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "bigram\n",
        "\"\"\"\n",
        "# Step 1: Count bigram frequencies for each sentence\n",
        "bigram_counts_list = [Counter(zip(sentence, sentence[1:])) for sentence in tokenized_data_with_unk_2dim]\n",
        "bigram_counts = Counter()\n",
        "for count in bigram_counts_list:\n",
        "    bigram_counts.update(count)\n",
        "\n",
        "# Step 3: Calculate add-one smoothed probabilities for each sentence\n",
        "# vocab_sizes = [len(unigram_count) for unigram_count in unigram_counts]\n",
        "# total_bigrams = [sum(bigram_count.values()) for bigram_count in bigram_counts]\n",
        "\n",
        "def calculate_bigram_prob(word1, word2):\n",
        "    return bigram_counts[(word1, word2)] / unigram_counts[word1]\n",
        "\n",
        "# Step 4: Calculate perplexity for each sentence\n",
        "def calculate_perplexity(sentence):\n",
        "    num_words = len(sentence)\n",
        "    log_prob_sum = 0\n",
        "\n",
        "    for i in range(1, num_words):\n",
        "        word1 = sentence[i - 1]\n",
        "        word2 = sentence[i]\n",
        "        prob = calculate_bigram_prob(word1, word2)\n",
        "        log_prob_sum += np.log(prob)\n",
        "\n",
        "    perplexity = np.exp(log_prob_sum) ** (-1/num_words)\n",
        "    return perplexity\n",
        "\n",
        "# Calculate perplexity for each sentence\n",
        "sentence_perplexities = []\n",
        "for i, sentence in enumerate(tokenized_data_with_unk_2dim):\n",
        "    perplexity = calculate_perplexity(sentence)\n",
        "    sentence_perplexities.append(perplexity)\n",
        "\n",
        "print(\"Perplexity for each sentence:\")\n",
        "for i, perplexity in enumerate(sentence_perplexities):\n",
        "    print(f\"Sentence {i+1}: {perplexity}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # bigram\n",
        "\n",
        "# from collections import defaultdict\n",
        "# # calc conditional perp\n",
        "# biProbs = defaultdict(lambda: defaultdict(int))\n",
        "# for wi in biCnt:\n",
        "#     wiCnt = 0\n",
        "#     for _, cnt in biCnt[wi].items():\n",
        "#         wiCnt += cnt\n",
        "#     for wj, cnt in biCnt[wi].items():\n",
        "#         biProbs[wi][wj] = cnt / wiCnt\n",
        "# print(biProbs['<s>'])\n",
        "# print(biProbs['The'])\n",
        "# assert sum(biProbs['<s>'].values()) - 1 < 1e-5\n",
        "# assert sum(biProbs['The'].values()) - 1 < 1e-5\n",
        "\n",
        "# # Calculate the perp of each sentence and then calculate the mean\n",
        "# def calc_perp(biProbs):\n",
        "#     perps = []\n",
        "#     with open(f'data/lm/{dataset}.txt', encoding='utf-8') as fd:\n",
        "#         lines = fd.readlines()\n",
        "#         for line in lines:\n",
        "#             words = line.split(' ')\n",
        "#             perp = biProbs['<s>'][words[0]] ** (-1/len(words))\n",
        "#             for i in range(1, len(words)):\n",
        "#                 if biProbs[words[i - 1]][words[i]] != 0:\n",
        "#                     perp *= biProbs[words[i - 1]][words[i]] ** (-1/len(words))\n",
        "                \n",
        "#             perp *= biProbs[words[-1]]['</s>'] ** (-1/len(words))\n",
        "#             # perp = perp ** (-1/len(words))\n",
        "#             # print(perp)\n",
        "#             perps.append(perp)\n",
        "#     return perps\n",
        "    \n",
        "# perps = calc_perp(biProbs)\n",
        "# print(perps[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3o9Nez8kvYm"
      },
      "source": [
        "**Discussion**\n",
        "\n",
        "The lower the Perplexity value, the more accurate the model's prediction of the language sequence.\n",
        "\n",
        "Under the same corpus and model training conditions, the perplexity of the unigram model is higher and the perplexity of the bigram model is lower. Thus, the prediction of the bigram model is more accurate. This is because the bigram model takes into account neighboring The contextual information between words can capture local semantic and syntactic relationships better than the unigram model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOQUqM73kzf-"
      },
      "source": [
        "### 2.3 Smoothing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LgXRmJwk3Y-"
      },
      "source": [
        "#### 2.3.1 Add-one (Laplace) smoothing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFG7jCIRk7Qw"
      },
      "source": [
        "**Code**\n",
        "\n",
        "\\# todo\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from copy import deepcopy\n",
        "# def getOovLaplace():\n",
        "#     oovLaplace = deepcopy(biCnt)\n",
        "#     # 计算新的条件概率\n",
        "#     biLaplaceProbs = defaultdict(lambda: defaultdict(int))\n",
        "#     for wi in oovLaplace:\n",
        "#         # add one\n",
        "#         for wj in oovLaplace:\n",
        "#             oovLaplace[wi][wj] += 1\n",
        "#         wiCnt = 0\n",
        "#         for wj, cnt in oovLaplace[wi].items():\n",
        "#             wiCnt += cnt\n",
        "#         for wj, cnt in oovLaplace[wi].items():\n",
        "#             biLaplaceProbs[wi][wj] = cnt / wiCnt\n",
        "#         print(oovLaplace[wi])\n",
        "#         return\n",
        "# biLaplaceProbs = getOovLaplace()\n",
        "\n",
        "# # 计算新的perplexity\n",
        "# perps = calc_perp(biLaplaceProbs)\n",
        "# print(perps[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_add_one_smoothed_prob(word1, word2):\n",
        "    numerator = bigram_counts[(word1, word2)] + 1\n",
        "    denominator = unigram_counts[word1] + vocabulary_size\n",
        "    return numerator / denominator\n",
        "\n",
        "\n",
        "class Bigram():\n",
        "    def __init__(self):\n",
        "        self.prob_list = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "    def train(self):\n",
        "        for sentence in tokenized_data_with_unk_2dim:\n",
        "            num_words = len(sentence)\n",
        "\n",
        "            for i in range(1, num_words):\n",
        "                word1 = sentence[i - 1]\n",
        "                word2 = sentence[i]\n",
        "                prob = calculate_add_one_smoothed_prob(word1, word2)\n",
        "                self.prob_list[word1][word2] = prob\n",
        "\n",
        "    def get(self, word1, word2):\n",
        "        return self.prob_list[word1][word2]\n",
        "\n",
        "    def predict(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "bigram = Bigram()\n",
        "bigram.train()\n",
        "\n",
        "perplexity_train = []\n",
        "for sentence in tokenized_data_with_unk_2dim:\n",
        "    num_words = len(sentence)\n",
        "    log_prob_sum = 0\n",
        "\n",
        "    for i in range(1, num_words):\n",
        "        word1 = sentence[i - 1]\n",
        "        word2 = sentence[i]\n",
        "        perp = np.log(bigram.get(word1, word2))\n",
        "        log_prob_sum += perp\n",
        "\n",
        "    perplexity = np.exp(log_prob_sum) ** (-1/num_words)\n",
        "    perplexity_train.append(perplexity)\n",
        "\n",
        "print(\"Perplexity for each sentence:\")\n",
        "for i, perplexity in enumerate(perplexity_train):\n",
        "    print(f\"Sentence {i+1}: {perplexity}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36yTKPXFk8f2"
      },
      "source": [
        "**Discussion**\n",
        "\n",
        "perplexity increases\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8cFbczqlBR_"
      },
      "source": [
        "#### 2.3.2: Add-$k$ smoothing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uV_ZiAgIlPUu"
      },
      "source": [
        "**Code**\n",
        "\n",
        "\\# todo\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHFNf8OIlQ0O"
      },
      "source": [
        "**Discussion**\n",
        "\n",
        "\\# todo\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjKEO_TqlUrX"
      },
      "source": [
        "#### 2.3.3 Linear Interpolation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "trigram\n",
        "\"\"\"\n",
        "from collections import Counter\n",
        "\n",
        "tokenized_data = ['I', 'like', 'apples', 'and', 'bananas', 'I', 'like', 'apples']\n",
        "trigram_counts = Counter(zip(tokenized_data, tokenized_data[1:], tokenized_data[2:]))\n",
        "print(trigram_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lambda1 = 0.2  # Weight for unigram\n",
        "lambda2 = 0.4  # Weight for bigram\n",
        "lambda3 = 0.4  # Weight for trigram\n",
        "\n",
        "def linear_interpolation_smoothing(word1, word2, word3):\n",
        "    # Calculate the interpolated probability using weighted averages\n",
        "    unigram_prob = unigram_model.get(word3, 0.0)\n",
        "    bigram_prob = bigram.get((word2, word3), 0.0)\n",
        "    trigram_prob = trigram_model.get((word1, word2, word3), 0.0)\n",
        "\n",
        "    interpolated_prob = lambda1 * unigram_prob + lambda2 * bigram_prob + lambda3 * trigram_prob\n",
        "    return interpolated_prob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcdd4cvYlZuO"
      },
      "source": [
        "**Code**\n",
        "\n",
        "\\# todo\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyKqmQ37lcH2"
      },
      "source": [
        "**Discussion**\n",
        "\n",
        "\\# todo\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzSbk2bClf3u"
      },
      "source": [
        "##### **Optimization**:\n",
        "\n",
        "\\# todo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgTcTlLuloHu"
      },
      "source": [
        "## 3 Preposition Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jb0OQ-yltc3"
      },
      "outputs": [],
      "source": [
        "!wget -O dev.in https://raw.githubusercontent.com/qtli/COMP7607-Fall2023/master/assignments/A1/data/prep/dev.in\n",
        "!wget -O dev.out https://github.com/qtli/COMP7607-Fall2023/blob/master/assignments/A1/data/prep/dev.out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "read dev.in & prep by bigram\n",
        "\"\"\"\n",
        "prep_file = []\n",
        "with open(f'data/prep/dev.in', encoding='utf-8') as fd:\n",
        "    with open('data/prep/prep.out', 'w', encoding='utf-8') as file:\n",
        "        lines = fd.readlines()\n",
        "        for line in lines:\n",
        "            words = line.split(' ')\n",
        "            \n",
        "            prep_line = []\n",
        "            for i in range(len(words)):\n",
        "                if (words[i] == '<PREP>'):\n",
        "                    dic = bigram.prob_list[words[i - 1]]\n",
        "                    # cope with UNK\n",
        "                    if words[i - 1] not in vocabulary or not dic:\n",
        "                        dic = bigram.prob_list[\"<UNK>\"]\n",
        "\n",
        "                    # prep by bigram\n",
        "                    prep = max(dic, key=dic.get)\n",
        "                    prep_line.append(prep)\n",
        "            \n",
        "            prep_file.append(\" \".join(prep_line))\n",
        "\n",
        "            file.write(\" \".join(prep_line) + '\\n')\n",
        "\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "864 1327104\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "calculate the accuracy of the bigram\n",
        "\"\"\"\n",
        "total = 0\n",
        "right = 0\n",
        "with open(f'data/prep/dev.out', encoding='utf-8') as fd_dev:\n",
        "    with open('data/prep/prep.out', encoding='utf-8') as fd_prep:\n",
        "        lines_dev = fd_dev.readlines()\n",
        "        lines_prep = fd_prep.readlines()\n",
        "        for i in range(len(lines_dev)):\n",
        "            line_dev = lines_dev[i].strip().split()\n",
        "            line_prep = lines_prep[i].strip().split()\n",
        "\n",
        "            for j in range(len(line_dev)):\n",
        "                right += line_dev[j] == line_prep[j]\n",
        "            total += len(lines_dev)\n",
        "print(right, total)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
